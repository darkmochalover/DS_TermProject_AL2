{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "170niz0dc6ppbZXOXwv-WMJ5ASnrtNoma",
      "authorship_tag": "ABX9TyPkAAH/O85nuK0t7kGFWpd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkmochalover/DS_TermProject_AL2/blob/main/wandb_hp_baseline_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary"
      ],
      "metadata": {
        "id": "wzOh70v4rFn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Path\n",
        "raw_data_path = \"/content/drive/MyDrive/DS_TermProject/data/combined_mbti_df.csv\"\n",
        "\n",
        "audio_qualities = [\n",
        "    'danceability_mean',\n",
        "    'valence_mean',\n",
        "    'energy_mean',\n",
        "    'loudness_mean',\n",
        "    'acousticness_mean',\n",
        "    'instrumentalness_mean',\n",
        "    'liveness_mean',\n",
        "]\n",
        "\n",
        "# 장조/단조 (Major/Minor)\n",
        "all_tones = [\n",
        "    'Cminor_count', 'CMajor_count', 'C#/Dbminor_count', 'C#/DbMajor_count',\n",
        "    'DMajor_count', 'D#_EbMajor_count', 'Eminor_count', 'EMajor_count',\n",
        "    'Fminor_count', 'FMajor_count', 'F#/Gbminor_count', 'GMajor_count',\n",
        "    'G#/Abminor_count', 'G#/AbMajor_count', 'Aminor_count', 'AMajor_count',\n",
        "    'A#/Bbminor_count', 'BMajor_count', 'Dminor_count', 'D#_Ebminor_count',\n",
        "    'Gminor_count', 'A#/BbMajor_count', 'F#/GbMajor_count', 'Bminor_count'\n",
        "]\n",
        "\n",
        "major_tones = [\n",
        "    'CMajor_count', 'C#/DbMajor_count',\n",
        "    'DMajor_count', 'D#_EbMajor_count', \n",
        "    'EMajor_count',\n",
        "    'FMajor_count', \n",
        "    'GMajor_count', 'G#/AbMajor_count', \n",
        "    'AMajor_count', 'BMajor_count', 'A#/BbMajor_count', \n",
        "    'F#/GbMajor_count'\n",
        "]\n",
        "minor_tones = [\n",
        "    'Cminor_count', 'C#/Dbminor_count', \n",
        "    'Eminor_count', \n",
        "    'Fminor_count', 'F#/Gbminor_count', \n",
        "    'G#/Abminor_count',  \n",
        "    'Aminor_count', 'A#/Bbminor_count', \n",
        "    'Dminor_count', 'D#_Ebminor_count',\n",
        "    'Gminor_count', \n",
        "    'Bminor_count'\n",
        "]\n",
        "\n",
        "\n",
        "# 열 이름을 바꾸기 위해 리스트로 저장함\n",
        "renamed_columns =  [\n",
        "    'danceability',\n",
        "    'valence',\n",
        "    'energy',\n",
        "    'loudness',\n",
        "    'acousticness',\n",
        "    'instrumentalness',\n",
        "    'liveness'\n",
        "]\n"
      ],
      "metadata": {
        "id": "VweWag3WrIrd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install WandB"
      ],
      "metadata": {
        "id": "cu1JoFEDqIhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wandb"
      ],
      "metadata": {
        "id": "_MaTXzH4q4LN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Jm56g6Ep9Rg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import itertools\n",
        "\n",
        "import wandb\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import W&B and Login"
      ],
      "metadata": {
        "id": "HEXBn0GGqLDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback"
      ],
      "metadata": {
        "id": "NSHvaPp4q0IH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading"
      ],
      "metadata": {
        "id": "XUas7Lziryfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(raw_data_path)"
      ],
      "metadata": {
        "id": "8rCuN45Ur5kJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Reduction"
      ],
      "metadata": {
        "id": "thkPZL0ur63t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset only measures of centers\n",
        "X = df.iloc[: , :22] # 처음 22개 열을 선택해서 저장\n",
        "X = df[audio_qualities] # 오디오 품질과 해당 열이 있는 열을 선택해서 저장\n",
        "\n",
        "\n",
        "categories = renamed_columns[:]\n",
        "X.columns = renamed_columns\n",
        "\n",
        "\n",
        "# 장조/단조의 개수의 합을 계산해서 저장 (C장조, D단조, .. 이렇게 따로 계산되는거 말고, 위에 지정된 list 이용해서 sum값 넣어줌)\n",
        "X['major_count'] = df[major_tones].sum(axis=1).astype('int64')\n",
        "X['minor_count'] = df[minor_tones].sum(axis=1).astype('int64')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOE8tiZwsEwS",
        "outputId": "4268a63b-fc52-4998-8ae5-cd3b5165fee0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-482ca4c8bc7c>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['major_count'] = df[major_tones].sum(axis=1).astype('int64')\n",
            "<ipython-input-7-482ca4c8bc7c>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['minor_count'] = df[minor_tones].sum(axis=1).astype('int64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding & Scaling Function"
      ],
      "metadata": {
        "id": "xQFGTwxasIyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Encoding(df, encoding_method):\n",
        "    df = df.copy()\n",
        "\n",
        "    if(encoding_method == 'LabelEncoder'):\n",
        "        encoder = LabelEncoder()\n",
        "        target = encoder.fit_transform(df[['mbti']])\n",
        "        \n",
        "\n",
        "    if(encoding_method == 'OneHotEncoder'):\n",
        "        encoder = OneHotEncoder(sparse=False)\n",
        "        target = encoder.fit_transform(df[['mbti']])\n",
        "\n",
        "    return target\n",
        "\n",
        "def Scaling(scale_method, X_train, X_test):\n",
        "    if( scale_method == 'No Scale'):\n",
        "        return X_train, X_test\n",
        "\n",
        "    elif(scale_method == 'StandardScaler'):\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "    elif(scale_method == 'MinMaxScaler'):\n",
        "        scaler = MinMaxScaler()\n",
        "\n",
        "    elif(scale_method == 'RobustScaler'):\n",
        "        scaler = RobustScaler()\n",
        "\n",
        "\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.fit_transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled\n"
      ],
      "metadata": {
        "id": "acXdN4w3rzhr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "바꿔가면서 돌리기"
      ],
      "metadata": {
        "id": "J_tJBdoVYHEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "LTiOGZW2eAjf",
        "outputId": "e712c1bc-6a9e-4247-c6ec-8bd24cd992ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# os.rmdir('/content/wandb')\n",
        "shutil.rmtree('/content/wandb', ignore_errors=True)"
      ],
      "metadata": {
        "id": "MxzoA2wwfEKI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_config = {\n",
        "    'encoder' : 'LabelEncoder',\n",
        "    'scaler' : 'StandardScaler',\n",
        "    'test_size' : 0.2,\n",
        "    \"do_smote\" : 'True'\n",
        "}"
      ],
      "metadata": {
        "id": "mcgDjVZwkyYa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_list = ['LabelEncoder', 'OneHotEncoder']\n",
        "scaler_list = ['StandardScaler', 'MinMaxScaler', 'RobustScaler', 'No Scale']\n",
        "test_size_list = [0.3, 0.2, 0.1]\n",
        "do_smote = ['True', 'False']\n",
        "model_list = ['DecisionTreeClassifier']\n",
        "SearchMethods = ['Base', 'Grid', 'Random']"
      ],
      "metadata": {
        "id": "smB98NV7qFna"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grid search(hyperparameter)\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# random search(hyperparameter)\n",
        "param_dist = {\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}"
      ],
      "metadata": {
        "id": "gqs_LAvo2vw6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "combinations = list(itertools.product(encoder_list, scaler_list, do_smote, model_list, SearchMethods))\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3ZSTvRpX_PK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for encoder, scaler, do_smote, model, search_method in combinations:\n",
        "  # wandb.init(project=\"MBTI_playlist\", config=base_config)\n",
        "\n",
        "  config = {\n",
        "            'encoder' : encoder,\n",
        "            'scaler' : scaler,\n",
        "            'test_size' : 0.3,\n",
        "            \"do_smote\" : do_smote,\n",
        "            'model' : model,\n",
        "            'search_method' : search_method\n",
        "  }\n",
        "\n",
        "  wandb.init(project='MBTI_playlist_test005', entity='ds_2023_spring', config=config)\n",
        "\n",
        "  config = wandb.config\n",
        "  wandb.config.update(config)\n",
        "  print(config)\n",
        "\n",
        "\n",
        "  y = Encoding(df = df, encoding_method=encoder)\n",
        "  # print(y[:5])\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 5, test_size = 0.2)\n",
        "\n",
        "  X_train, X_test = Scaling(scale_method=scaler, X_train=X_train, X_test=X_test)\n",
        "\n",
        "  if(do_smote == 'True'): # If true,\n",
        "      smote = SMOTE(sampling_strategy='auto', random_state=0)\n",
        "      X_train, y_train = smote.fit_resample(X_train,y_train)\n",
        "\n",
        "  # 모델 평가 지표\n",
        "  wandb.define_metric('accuracy', summary='max')\n",
        "  wandb.define_metric('F1 Score', summary='max')\n",
        "  wandb.define_metric('Recall', summary='max')\n",
        "  wandb.define_metric('Precision', summary='max')\n",
        "\n",
        "  if(model == 'DecisionTreeClassifier'):\n",
        "    # (Base) Decision Tree Model\n",
        "    base_model = DecisionTreeClassifier(random_state = 42)\n",
        "\n",
        "    if(search_method == 'Grid'):\n",
        "      # Grid Search\n",
        "      grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=5)\n",
        "      grid_search.fit(X_train, y_train)\n",
        "\n",
        "      best_params_grid = grid_search.best_params_\n",
        "      dt_model_grid = DecisionTreeClassifier(random_state=42, **best_params_grid)\n",
        "      dt_model_grid.fit(X_train, y_train)\n",
        "      dt_predictions_grid = dt_model_grid.predict(X_test)\n",
        "      dt_accuracy_grid = accuracy_score(y_test, dt_predictions_grid)\n",
        "      dt_confusion_matrix_grid = confusion_matrix(y_test, dt_predictions_grid)\n",
        "      dt_f1_score_grid = f1_score(y_test, dt_predictions_grid, average='macro')\n",
        "      dt_recall_grid = recall_score(y_test, dt_predictions_grid, average='macro')\n",
        "      dt_precision_grid = precision_score(y_test, dt_predictions_grid, average='macro')\n",
        "      print(\"Decision Tree Accuracy with Grid Search:\", dt_accuracy_grid)\n",
        "      print(\"Decision Tree Confusion Matrix with Grid Search:\\n\", dt_confusion_matrix_grid)\n",
        "      print(\"Decision Tree F1 Score with Grid Search:\", dt_f1_score_grid)\n",
        "      print(\"Decision Tree Recall with Grid Search:\", dt_recall_grid)\n",
        "      print(\"Decision Tree Precision with Grid Search:\", dt_precision_grid)\n",
        "\n",
        "      wandb.log({'accuracy': dt_accuracy_grid})\n",
        "      wandb.log({'Confusion Matrix': dt_confusion_matrix_grid.tolist()})\n",
        "      wandb.log({'F1 Score': dt_f1_score_grid})\n",
        "      wandb.log({'Recall': dt_recall_grid})\n",
        "      wandb.log({'Precision': dt_precision_grid})\n",
        "        \n",
        "\n",
        "      # K-Fold \n",
        "      kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "      dt_model_cv = DecisionTreeClassifier(random_state=42, **best_params)\n",
        "      cv_scores = cross_val_score(dt_model_cv, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "      cv_accuracy = np.mean(cv_scores)\n",
        "      print(\"acc with K-Fold Cross Validation:\", cv_accuracy)\n",
        "      wandb.log({'Accuracy with K-Fold': cv_accuracy})  \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    if(search_method == 'Random Search'):\n",
        "      # Random Search\n",
        "      random_search = RandomizedSearchCV(estimator=dt_model, param_distributions=param_dist, cv=5)\n",
        "      random_search.fit(X_train, y_train)\n",
        "\n",
        "      best_params_random = random_search.best_params_\n",
        "      dt_model_random = DecisionTreeClassifier(random_state=42, **best_params_random)\n",
        "      dt_model_random.fit(X_train, y_train)\n",
        "      dt_predictions_random = dt_model_random.predict(X_test)\n",
        "      dt_accuracy_random = accuracy_score(y_test, dt_predictions_random)\n",
        "      dt_confusion_matrix_random = confusion_matrix(y_test, dt_predictions_random)\n",
        "      dt_f1_score_random = f1_score(y_test, dt_predictions_random, average='macro')\n",
        "      dt_recall_random = recall_score(y_test, dt_predictions_random, average='macro')\n",
        "      dt_precision_random = precision_score(y_test, dt_predictions_random, average='macro')\n",
        "      print(\"Decision Tree Accuracy with Random Search:\", dt_accuracy_random)\n",
        "      print(\"Decision Tree Confusion Matrix with Random Search:\\n\", dt_confusion_matrix_random)\n",
        "      print(\"Decision Tree F1 Score with Random Search:\", dt_f1_score_random)\n",
        "      print(\"Decision Tree Recall with Random Search:\", dt_recall_random)\n",
        "      print(\"Decision Tree Precision with Random Search:\", dt_precision_random)\n",
        "\n",
        "      wandb.log({'accuracy': dt_accuracy_random})\n",
        "      wandb.log({'Confusion Matrix': dt_confusion_matrix_random.tolist()})\n",
        "      wandb.log({'F1 Score': dt_f1_score_random})\n",
        "      wandb.log({'Recall': dt_recall_random})\n",
        "      wandb.log({'Precision': dt_precision_random})\n",
        "\n",
        "    \n",
        "      # K-Fold \n",
        "      kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "      dt_model_cv = DecisionTreeClassifier(random_state=42, **best_params)\n",
        "      cv_scores = cross_val_score(dt_model_cv, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "      cv_accuracy = np.mean(cv_scores)\n",
        "      print(\"acc with K-Fold Cross Validation:\", cv_accuracy)\n",
        "      wandb.log({'Accuracy with K-Fold': cv_accuracy})  \n"
      ],
      "metadata": {
        "id": "ha4l7Oqbr4fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "D3GKXG9Pyqk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}